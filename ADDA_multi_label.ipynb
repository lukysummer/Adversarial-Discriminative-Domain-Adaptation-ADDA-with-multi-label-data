{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADDA_multi_label.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d4b2731cf7146b2b9b9bc306bbb5d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ebc8aa5fc4f14ba1bad148ed421f20f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_56f1ef1d8ba24e04b23f8f5437388018",
              "IPY_MODEL_5e645311fbf3499a98bc3fe6409a97c2",
              "IPY_MODEL_b6232e1415e44d1ba670b67f2c797d1e"
            ]
          }
        },
        "ebc8aa5fc4f14ba1bad148ed421f20f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56f1ef1d8ba24e04b23f8f5437388018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_972f8e0cdb034cc9ae3df07a8254f232",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f856084f3a847a081192c13d7832745"
          }
        },
        "5e645311fbf3499a98bc3fe6409a97c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_456753721cc342ea842c4e9a285b271a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f0aaf78a8584062b14c40d6f0ff7f06"
          }
        },
        "b6232e1415e44d1ba670b67f2c797d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26d38cc43df74ebc919197744535fe8d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 248MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6679c3165e6841659b8c4fafc986c7c6"
          }
        },
        "972f8e0cdb034cc9ae3df07a8254f232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f856084f3a847a081192c13d7832745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "456753721cc342ea842c4e9a285b271a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f0aaf78a8584062b14c40d6f0ff7f06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26d38cc43df74ebc919197744535fe8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6679c3165e6841659b8c4fafc986c7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Written by** : Lucrece (Jahyun) Shin\n",
        "\n",
        "**Final Edit Date** : 12/22/2021\n",
        "\n",
        "**Contact** : lucrece.shin@mail.utoronto.ca\n",
        "# 1 Import Libraries"
      ],
      "metadata": {
        "id": "qwqEis0yV34B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os\n",
        "from PIL import Image, ImageFile\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "try:\n",
        "  from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "  from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "1MWi2bJMTb_B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your Google drive if necessary\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9_XtLkge99O",
        "outputId": "7b5642b6-f5b1-4760-b24d-46b72582fbeb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Define Multi-label Dataset\n",
        "To use multi-label dataset as discussed in [my blog post](https://medium.com/mlearning-ai/ch-6-optimizing-data-for-flexible-and-robust-image-recognition-23f4dcce3af7#f4d3), you must organize your folders in the following way:\n",
        "\n",
        "* [class a]+[class b] for images that contain both class a and b\n",
        "* [class a] for image that contain only class a\n",
        "\n",
        "For example, I originlly had 3 classes (gun, knife, benign) and made the following folders:\n",
        "* gun\n",
        "* gun+benign\n",
        "* knife\n",
        "* knife+benign\n",
        "* gun+knife\n",
        "* benign\n",
        "\n",
        "Here I am only considering images that contain **at most 2 classes**. Please adjust code for a situation where you must consider image containing more than 2 classes."
      ],
      "metadata": {
        "id": "lc-CHsGtRzuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLabelWebDataset(Dataset):\n",
        "  def __init__(self, root_dir, classes, transform=None, soft_label_class_name=None, soft_label=0.5):\n",
        "    # soft_label_class_name : name of the class that is given a soft label < 1\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.classes = classes\n",
        "    self.class_to_idx = {c:i for i, c in enumerate(self.classes)}\n",
        "    self.soft_label_class_name = soft_label_class_name\n",
        "    self.soft_label = soft_label  \n",
        "    self.data = self.make_dataset()                                        \n",
        "                                                      \n",
        "  def __len__(self):\n",
        "    return len(self.make_dataset())\n",
        "\n",
        "  def make_dataset(self):\n",
        "    instances = []\n",
        "    for target_class in os.listdir(self.root_dir):      \n",
        "      target_dir = os.path.join(self.root_dir, target_class)\n",
        "      # split up the class names by \"+\" sign\n",
        "      class_names = target_class.split(\"+\") # list of length 1 or 2\n",
        "      if not os.path.isdir(target_dir):\n",
        "        continue\n",
        "      for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "        for fname in sorted(fnames):  # for each image\n",
        "          label = [0]*len(self.classes)\n",
        "          path = os.path.join(self.root_dir, target_class, fname)\n",
        "\n",
        "          if len(class_names)==1:  # images that contain only one class\n",
        "            single_cls = class_names[0]\n",
        "            if single_cls==self.soft_label_class_name:\n",
        "              label[self.class_to_idx[single_cls]] = self.soft_label\n",
        "            else:\n",
        "              label[self.class_to_idx[single_cls]] = 1.\n",
        "\n",
        "          elif len(class_names)==2:  # images that contain two classes\n",
        "            for cls in class_names:\n",
        "              if cls==self.soft_label_class_name:\n",
        "                label[self.class_to_idx[cls]] = self.soft_label      \n",
        "              else:\n",
        "                label[self.class_to_idx[cls]] = 1.\n",
        "          item = path, label\n",
        "          instances.append(item)\n",
        "    return instances\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    path, target = self.data[idx]\n",
        "    image = Image.open(path).convert('RGB') \n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image, torch.tensor(target)"
      ],
      "metadata": {
        "id": "mRuqZ-lFs1cZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp 'gdrive/My Drive/web_3cls+NOcutter.zip' . \n",
        "!unzip -qq web_3cls+NOcutter.zip\n",
        "!rm web_3cls+NOcutter.zip "
      ],
      "metadata": {
        "id": "W0K5_lPsXHsm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Define Source Domain Dataloader"
      ],
      "metadata": {
        "id": "61BUE3kUeR5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define image transform\n",
        "transform_ = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                 transforms.RandomHorizontalFlip(),\n",
        "                                 transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "2I8ipYoIXjQx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `src_root_dir` is a string that contains the name of your **source domain data folder**. It must contain 3 folders named 'train', 'valid', 'test', each containing folders named by class names.\n",
        "* **soft_label** is a weaker label less than 1. Usually a class is given a label of 1 for a one-hot-encoded label. If a less significant class (if any) is recieving a strong signal from the model (i.e. large recall), you can set a soft label of e.g. 0.5 instead of 1 for that class."
      ],
      "metadata": {
        "id": "2XHTNQfSXpmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_root_dir = \"web_3cls+NOcutter\"## insert your data folder here ##\n",
        "classes=['benign','gun','knife']\n",
        "soft_label_class_name=\"benign\"\n",
        "soft_label=0.5 # weaker label < 1 \n",
        "train_data_multi_label = MultiLabelWebDataset(src_root_dir + '/train', classes=classes, transform = transform_, soft_label_class_name=soft_label_class_name, soft_label=soft_label)\n",
        "valid_data_multi_label = MultiLabelWebDataset(src_root_dir + '/valid', classes=classes, transform = transform_, soft_label_class_name=soft_label_class_name, soft_label=soft_label)\n",
        "test_data_multi_label = MultiLabelWebDataset(src_root_dir + '/test', classes=classes, transform = transform_, soft_label_class_name=soft_label_class_name, soft_label=soft_label)\n",
        "print(\"Class2idx: \", train_data_multi_label.class_to_idx)\n",
        "num_workers = 0\n",
        "batch_size = 16\n",
        "dataloaders_multi_label = {}\n",
        "dataloaders_multi_label['train'] = DataLoader(train_data_multi_label, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "dataloaders_multi_label['valid'] = DataLoader(valid_data_multi_label, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "dataloaders_multi_label['test'] = DataLoader(test_data_multi_label, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "print('Train images :', len(train_data_multi_label), \", # of training batches:\", len(dataloaders_multi_label['train']))\n",
        "print('Valid images :', len(valid_data_multi_label), \", # of valid batches:\", len(dataloaders_multi_label['valid']))\n",
        "print('Test images :', len(test_data_multi_label), \", # of test batches:\", len(dataloaders_multi_label['test']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sjp-1W5szUf",
        "outputId": "4427b479-5fef-4c57-d1f9-8a7b8f95e7c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class2idx:  {'benign': 0, 'gun': 1, 'knife': 2}\n",
            "Train images : 5024 , # of training batches: 314\n",
            "Valid images : 645 , # of valid batches: 41\n",
            "Test images : 627 , # of test batches: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Define Target Domain Dataloader"
      ],
      "metadata": {
        "id": "UZ0KJRkUeVdv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53E39fDVZGhW"
      },
      "source": [
        "!cp gdrive/MyDrive/Xray_2classes_cropped.zip .\n",
        "!unzip -qq Xray_2classes_cropped.zip\n",
        "!rm Xray_2classes_cropped.zip\n",
        "!rm Xray_2classes_cropped/knife/Kitchen*\n",
        "!rm Xray_2classes_cropped/knife/Cutter*\n",
        "!cp gdrive/My Drive/Xray-3cls_small.zip .\n",
        "!unzip -qq Xray-3cls_small.zip\n",
        "!rm Xray-3cls_small.zip \n",
        "!rm Xray-3cls_small/knife/Kitchen* \n",
        "!rm Xray-3cls_small/knife/Cutter* "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Define Target Domain **Train** Dataloader"
      ],
      "metadata": {
        "id": "eDtGj0DZUkE_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb_niWVr-aZy",
        "outputId": "639648b9-9184-4030-ed6f-d59fba35911d"
      },
      "source": [
        "num_workers = 0\n",
        "batch_size = 16\n",
        "tgt_root_dir = 'Xray_2classes_cropped'\n",
        "xray_dataset = datasets.ImageFolder(tgt_root_dir, transform = transform_) # resize only\n",
        "xray_dataloader = DataLoader(xray_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "print(\"Classes: \", xray_dataset.class_to_idx)\n",
        "print(\"Number of Xray images : {}    Number of Xray batches : {}  (batch size={})\".format(len(xray_dataloader.dataset), len(xray_dataloader), batch_size))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes:  {'benign_inch': 0, 'gun': 1, 'knife': 2}\n",
            "Number of Xray images : 1850    Number of Xray batches : 116  (batch size=16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Define Target Domain **Test** Dataloader (smaller)"
      ],
      "metadata": {
        "id": "KUQzjUm-UpZW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6QemFTyQ2BF",
        "outputId": "503aeee6-6f6e-4e12-dc16-667ca14cd680"
      },
      "source": [
        "tgt_root_dir_test = 'Xray-3cls_small'\n",
        "xray_test_dataset = datasets.ImageFolder(tgt_root_dir_test, transform = transform_) # resize only\n",
        "xray_test_dataloader = DataLoader(xray_test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "print(\"Classes: \", xray_test_dataset.class_to_idx)\n",
        "print(\"Number of Xray images : {}    Number of Xray batches : {}  (batch size={})\".format(len(xray_test_dataset), len(xray_test_dataloader), batch_size))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes:  {'benign_inch': 0, 'gun': 1, 'knife': 2}\n",
            "Number of Xray images : 450    Number of Xray batches : 29  (batch size=16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Define Combined Dataloader\n",
        "This is exclusively used for plotting **t-SNE plots** of source domain and target domain features together. "
      ],
      "metadata": {
        "id": "iY0br8OtfMAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir combined_folder\n",
        "!mkdir combined_folder/gun_xray\n",
        "!mkdir combined_folder/knife_xray\n",
        "!mkdir combined_folder/gun_web\n",
        "!mkdir combined_folder/knife_web\n",
        "\n",
        "!cp web_3cls+NOcutter/test/gun/* combined_folder/gun_web/\n",
        "!cp web_3cls+NOcutter/test/knife/* combined_folder/knife_web/\n",
        "!cp Xray-3cls_small/gun/* combined_folder/gun_xray/\n",
        "!cp Xray-3cls_small/knife/* combined_folder/knife_xray/\n",
        "!mkdir combined_folder/benign_xray\n",
        "!cp Xray-3cls_small/benign_inch/* combined_folder/benign_xray/"
      ],
      "metadata": {
        "id": "iHDpP3FAfSO-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 classes : gun, knife, benign\n",
        "data_path = 'combined_folder'\n",
        "num_workers = 0\n",
        "batch_size = 16\n",
        "\n",
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "  ''' dataset containing images as well as image filenames '''\n",
        "  def __getitem__(self, index):\n",
        "    original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "    path = self.imgs[index][0]\n",
        "    tuple_with_path = (original_tuple + (path,))\n",
        "    return tuple_with_path\n",
        "\n",
        "combined_dataset = ImageFolderWithPaths(data_path, transform = transform_) \n",
        "combined_dataloader = DataLoader(combined_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "print(\"Classes: \", combined_dataset.classes)\n",
        "print(\"Number of Xray images : {}    Number of Xray batches : {}  (batch size={})\".format(len(combined_dataset), len(combined_dataloader), batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOw7whYFfLj8",
        "outputId": "7818a3a8-9ce4-48bd-e1f9-c807bc94eb23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes:  ['benign_xray', 'gun_web', 'gun_xray', 'knife_web', 'knife_xray']\n",
            "Number of Xray images : 826    Number of Xray batches : 52  (batch size=16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Define Model-related Functions/Classes"
      ],
      "metadata": {
        "id": "91OoAfROTruV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Define Function to load ResNet50 pre-trained on Stylized ImageNet \n",
        "From paper : [ImageNet-trained CNNs are biased towards texture](https://arxiv.org/abs/1811.12231) and the author's [Github repo](https://github.com/rgeirhos/texture-vs-shape)\n"
      ],
      "metadata": {
        "id": "CorPAz4Nxfwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_stylized_imagenet(model_name):\n",
        "\n",
        "    model_urls = {\n",
        "            'resnet50_trained_on_SIN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/6f41d2e86fc60566f78de64ecff35cc61eb6436f/resnet50_train_60_epochs-c8e5653e.pth.tar',\n",
        "            'resnet50_trained_on_SIN_and_IN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/60b770e128fffcbd8562a3ab3546c1a735432d03/resnet50_train_45_epochs_combined_IN_SF-2a0d100e.pth.tar',\n",
        "            'resnet50_trained_on_SIN_and_IN_then_finetuned_on_IN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/60b770e128fffcbd8562a3ab3546c1a735432d03/resnet50_finetune_60_epochs_lr_decay_after_30_start_resnet50_train_45_epochs_combined_IN_SF-ca06340c.pth.tar',\n",
        "            'alexnet_trained_on_SIN': 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/0008049cd10f74a944c6d5e90d4639927f8620ae/alexnet_train_60_epochs_lr0.001-b4aa5238.pth.tar',\n",
        "    }\n",
        "    if \"resnet50\" in model_name:\n",
        "        print(\"Using the ResNet50 architecture.\")\n",
        "        model = torchvision.models.resnet50(pretrained=False)\n",
        "        model = torch.nn.DataParallel(model)#.cuda()\n",
        "        checkpoint = model_zoo.load_url(model_urls[model_name])\n",
        "\n",
        "    elif \"vgg16\" in model_name:\n",
        "        print(\"Using the VGG-16 architecture.\")\n",
        "        # download model from URL manually and save to desired location\n",
        "        filepath = \"./vgg16_train_60_epochs_lr0.01-6c6fcc9f.pth.tar\"\n",
        "        assert os.path.exists(filepath), \"Please download the VGG model yourself from the following link and save it locally: https://drive.google.com/drive/folders/1A0vUWyU6fTuc-xWgwQQeBvzbwi6geYQK (too large to be downloaded automatically like the other models)\"\n",
        "        model = torchvision.models.vgg16(pretrained=False)\n",
        "        model.features = torch.nn.DataParallel(model.features)\n",
        "        model.cuda()\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "    elif \"alexnet\" in model_name:\n",
        "        print(\"Using the AlexNet architecture.\")\n",
        "        model = torchvision.models.alexnet(pretrained=False)\n",
        "        model.features = torch.nn.DataParallel(model.features)\n",
        "        model.cuda()\n",
        "        checkpoint = model_zoo.load_url(model_urls[model_name])\n",
        "    else:\n",
        "        raise ValueError(\"unknown model architecture.\")\n",
        "\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "NAeOGj6WTuig"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Define Discriminator Class"
      ],
      "metadata": {
        "id": "bQdhCwugyJWV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU27NvGBg79P"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, feature_dim):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.restored = False\n",
        "    self.layer = nn.Sequential(\n",
        "      nn.Linear(feature_dim, 1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(1024, 2048),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(2048, 2),\n",
        "      nn.LogSoftmax()\n",
        "    )       \n",
        "  def forward(self, input):\n",
        "    out = self.layer(input)\n",
        "    return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Define t-SNE Plotting Function"
      ],
      "metadata": {
        "id": "vfTvND_2XUEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_to_01_range(x):  \n",
        "  ''' scale and move the coordinates so they fit [0; 1] range '''\n",
        "  value_range = (np.max(x) - np.min(x))\n",
        "  starts_from_zero = x - np.min(x)\n",
        "  return starts_from_zero / value_range\n",
        "\n",
        "def plot_tsne(dataloader, # contains data whose encoded features will be plotted\n",
        "              encoder,    # model to pass data through to extract features \n",
        "              plot_imgs=False, # for each feature, plot images if True, plot coloured dots if False\n",
        "              model_type='resnet50'): # supports one of 'resnet50' or 'vgg16'\n",
        "\n",
        "  assert model_type in ['resnet50', 'vgg16'], 'model_type must be one of \"resnet50\" or \"vgg16\"!'\n",
        "  encoder = encoder.cuda().eval()\n",
        "  for i, (data, target, fname) in enumerate(dataloader):\n",
        "    data = data.cuda() \n",
        "    with torch.no_grad():\n",
        "      if model_type == \"resnet50\":\n",
        "        outputs = encoder(data)\n",
        "        outputs = torch.flatten(outputs, 1)\n",
        "      elif model_type == \"vgg16\": \n",
        "        outputs = encoder.features(data)\n",
        "        outputs = encoder.avgpool(outputs)\n",
        "        outputs = torch.flatten(outputs, 1)\n",
        "        outputs = encoder.classifier[0](outputs)\n",
        "        outputs = encoder.classifier[1](outputs)\n",
        "        outputs = encoder.classifier[2](outputs)\n",
        "        outputs = encoder.classifier[3](outputs)\n",
        "        outputs = encoder.classifier[4](outputs)\n",
        "    outputs = outputs.cpu().numpy()\n",
        "    features = outputs if i==0 else np.concatenate((features, outputs), axis=0)  \n",
        "    labels = target if i==0 else np.concatenate((labels, target), axis=0)\n",
        "    fnames = list(fname) if i==0 else fnames + list(fname) \n",
        "\n",
        "  print(\"# of samples : {} \\n feature-dim : {}\".format(features.shape[0], features.shape[1]))\n",
        "  tsne = TSNE(n_components=2).fit_transform(features)\n",
        "\n",
        "  # extract x and y coordinates representing the positions of the images on T-SNE plot\n",
        "  fig = plt.figure(figsize=(8,5))\n",
        "  tx = scale_to_01_range(tsne[:, 0])\n",
        "  ty = scale_to_01_range(tsne[:, 1])\n",
        "  \n",
        "  classes = dataloader.dataset.classes \n",
        "  class2idx = {c:i for i, c in enumerate(classes)}\n",
        "  # define list of colours for coloured dots\n",
        "  colors = ['#00ffff', '#ff4000', '#ffbf00', '#0080ff', '#FF00FF', '#00ffff', '#008000', '#80ff00', '#8000ff', '#CCCCFF']\n",
        "  colors_per_class = {label:colors[i] for i, label in enumerate(classes)}\n",
        "  if plot_imgs:\n",
        "    width, height = 4000, 3000\n",
        "    max_dim = 100\n",
        "    full_image = Image.new('RGBA', (width, height))\n",
        "    img_paths = fnames\n",
        "\n",
        "  for label in colors_per_class:\n",
        "    indices = [i for i, l in enumerate(labels) if l == class2idx[label]]\n",
        "    current_tx = np.take(tx, indices)\n",
        "    current_ty = np.take(ty, indices)\n",
        "    \n",
        "    if plot_imgs:\n",
        "      current_img_paths = np.take(img_paths, indices)\n",
        "      for img, x, y in zip(current_img_paths, current_tx, current_ty):\n",
        "        tile = Image.open(img)\n",
        "        rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
        "        tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)\n",
        "        full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGBA'))\n",
        "    else:\n",
        "      color = colors_per_class[label]  \n",
        "      ax = fig.add_subplot(111)\n",
        "      ax.scatter(current_tx, current_ty, c=color, label=label, alpha=0.5)\n",
        "\n",
        "  if plot_imgs:\n",
        "    plt.figure(figsize = (16,12))\n",
        "    plt.imshow(full_image)\n",
        "  else:\n",
        "    ax.legend(loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VqJ0xePmsuA3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Define Confusion Matrix Calculating Function"
      ],
      "metadata": {
        "id": "BKLywDJAyPBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_confusion_matrix(encoder, \n",
        "                               classifier, \n",
        "                               transform, # torchvision.transforms object\n",
        "                               img_dir,   # name of directory containing images to be tested (to calculate confusion matrix)\n",
        "                               classes,   # list containing class names\n",
        "                               multi_label=True, # True for multi-label data, False for single-label data\n",
        "                               threshold=0.3):   # detection threshold for multi-label data\n",
        "  encoder.cuda().eval()\n",
        "  classifier.cuda().eval()\n",
        "  n_classes = len(classes)\n",
        "  test_labels = []\n",
        "  first_img = True\n",
        "  \n",
        "  # initialize confusion matrix \n",
        "  cm = np.zeros((n_classes, 3))\n",
        "  for cl in os.listdir(img_dir): # for each class\n",
        "    cls_i = classes.index(cl)\n",
        "    for img_path in os.listdir(os.path.join(img_dir, cl)): # for each image\n",
        "      test_labels.append(cls_i)\n",
        "      img = Image.open(os.path.join(img_dir, cl, img_path)).convert('RGB')     \n",
        "      img = transform(img)[:3, :, :].unsqueeze(0)    \n",
        "      img = img.cuda() if use_cuda else img\n",
        "      with torch.no_grad():\n",
        "        logits = encoder(img)\n",
        "        logits = torch.flatten(logits, 1)\n",
        "        logits = classifier(logits).cpu().detach().numpy()\n",
        "        \n",
        "      if multi_label:  # Multi-label Prediction \n",
        "        logits = logits[0]\n",
        "        pred = [1 if prob > threshold else 0 for i, prob in enumerate(logits)]\n",
        "        # if all logits < threshold, predict the one with the largest logit\n",
        "        if sum(pred)==0: \n",
        "          pred = [1 if i==np.argmax(logits) else 0 for i, prob in enumerate(logits)] \n",
        "        cm[cls_i] += pred\n",
        "      else:\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        test_preds = preds if first_img else np.concatenate((test_preds, preds), axis=0)  \n",
        "        first_img = False\n",
        "\n",
        "  if multi_label==False:\n",
        "    # using sklearn.metrics.confusion_matrix\n",
        "    cm = confusion_matrix(test_labels, test_preds, labels=np.arange(n_classes))\n",
        "\n",
        "  return cm"
      ],
      "metadata": {
        "id": "8HpEL21sEL0a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Define ADDA Training Function"
      ],
      "metadata": {
        "id": "__xd7MooXQ3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_variable(tensor, volatile=False):\n",
        "  ''' function to make tensor variable '''\n",
        "  if use_cuda:\n",
        "    tensor = tensor.cuda()\n",
        "  return Variable(tensor, volatile=volatile)"
      ],
      "metadata": {
        "id": "4tlOCSAuu7QF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5KWQLgYOsAP8"
      },
      "outputs": [],
      "source": [
        "def train_adda(num_epochs,  # number of epochs to train\n",
        "               lr,          # learning rate\n",
        "               save_step,   # save model checkpoint every this number of epochs \n",
        "               encoder,        \n",
        "               classifier,            \n",
        "               discriminator, \n",
        "               src_data_loaders,      # dict of train/valid/test source domain dataloaders\n",
        "               tgt_data_loader,       # target domain dataloader for training\n",
        "               tgt_data_loader_small, # target domain dataloader for testing (smaller)\n",
        "               combined_dataloader,   # source + target domain dataloader for plotting t-SNE\n",
        "               src_test_dir,   # file directory containing source doamin test images\n",
        "               tgt_test_dir,   # file directory containing source doamin test images\n",
        "               alpha_CLS=1.,      # coefficient for class classification loss in computing the total loss for encoder\n",
        "               alpha_DA=1.,       # coefficient for domain adaptation (confusion) loss in computing the total loss for encoder\n",
        "               multi_label=False, # True for multi-label data, False for single-label data\n",
        "               test_threshold=0.3 # output threshold for considering the object as present for multi-label data\n",
        "               ):\n",
        "  ''' ADDA Training using symmetric mapping '''\n",
        "  ### Define loss for class-classification ###\n",
        "  criterion_cls = nn.MSELoss() if multi_label else nn.CrossEntropyLoss() # MSE loss for soft-label \n",
        "\n",
        "  ### Define loss for domain-classification ###\n",
        "  criterion_DA  = nn.CrossEntropyLoss()\n",
        "\n",
        "  ### Define optimizers for encoder, classifier, and discriminator ###\n",
        "  optimizer_encoder       = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "  optimizer_classifier    = optim.Adam(classifier.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "  optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "  len_data_loader  = min(len(src_data_loaders['train']), len(tgt_data_loader)) # len(tgt_unlabeled_data_loader))\n",
        "  valid_loss_min = np.Inf\n",
        "  prev_save = \"\"\n",
        "\n",
        "  ### Move the models to GPU ###\n",
        "  if use_cuda:\n",
        "    discriminator.cuda()\n",
        "    encoder.cuda()\n",
        "    classifier.cuda()\n",
        "\n",
        "  classification_losses_E, domain_confusion_losses_E, losses_D, accs_D = [], [], [], []\n",
        "  train_loss_E_class, train_loss_E_domain, train_loss_D, train_acc_D = 0., 0., 0., 0.\n",
        "  valid_loss_class, val_n_corr_class = 0., 0 # only check for class-classification for validation (no domain-related tasks)\n",
        "\n",
        "  ### Start Training! ###\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    #### 1. Plot t-SNE plot with source and target domain features together ####\n",
        "    plot_tsne(combined_dataloader, encoder)\n",
        "\n",
        "    ### Start timing ###\n",
        "    start = time.time()       \n",
        "\n",
        "\n",
        "    ####################  2. Loop through Training batches  ####################\n",
        "    for step, ((images_src, tgt_src), (images_tgt, _)) in enumerate(zip(src_data_loaders['train'], tgt_data_loader)): \n",
        "      ##########################################################################\n",
        "      #######  2.1 Train Source Encoder & Classifier with class labels  ########\n",
        "      ##########################################################################\n",
        "      encoder.train()\n",
        "      classifier.train()\n",
        "      images_src, images_tgt = make_variable(images_src), make_variable(images_tgt)\n",
        "      tgt_src = tgt_src.type(torch.FloatTensor).cuda() if multi_label else make_variable(tgt_src)\n",
        "      optimizer_encoder.zero_grad()\n",
        "      optimizer_classifier.zero_grad()\n",
        "\n",
        "      ### Forward only SOURCE DOMAIN images through Encoder & Classifier ###\n",
        "      output = encoder(images_src)    # [batch_size, n_classes]  (target: [batch_size])\n",
        "      output = torch.flatten(output, 1)\n",
        "      output = classifier(output)\n",
        "\n",
        "      ### Calculate class-classification loss for Encoder and Classifier ###\n",
        "      loss_CLS = criterion_cls(output, tgt_src)\n",
        "      train_loss_E_class += loss_CLS.item() \n",
        "\n",
        "\n",
        "      ##########################################################################   \n",
        "      #############  2.2 Train Discriminator with domain labels  ###############\n",
        "      ##########################################################################\n",
        "      discriminator.train()\n",
        "      optimizer_discriminator.zero_grad()\n",
        "\n",
        "      ### Forward pass through Encoder ###\n",
        "      feat_src = encoder(images_src) \n",
        "      feat_tgt = encoder(images_tgt)\n",
        "      \n",
        "      ### Concatenate source domain and target domain features ###\n",
        "      feat_concat = torch.cat((feat_src, feat_tgt), 0) # [batch_size*2, 2048, 1, 1]\n",
        "      feat_concat = feat_concat.squeeze(-1).squeeze(-1)  # [batch_size*2, 2048]\n",
        "\n",
        "      ### Forward concatenated features through Discriminator ###\n",
        "      pred_concat = discriminator(feat_concat.detach())\n",
        "\n",
        "      ### prepare source domain labels (1) and target domain labels (0) ###\n",
        "      label_src = make_variable(torch.ones(feat_src.size(0)).long()) \n",
        "      label_tgt = make_variable(torch.zeros(feat_tgt.size(0)).long())\n",
        "      label_concat = torch.cat((label_src, label_tgt), 0)\n",
        "\n",
        "      ### Calculate domain-classification loss for Discriminator ###\n",
        "      loss_discriminator = criterion_DA(pred_concat.squeeze(1), label_concat)\n",
        "\n",
        "      ### Backward Propagation for Discriminator ###\n",
        "      loss_discriminator.backward()\n",
        "      optimizer_discriminator.step()\n",
        "\n",
        "      ### Update running losses/accuracies ###\n",
        "      train_loss_D += loss_discriminator.item()\n",
        "      \n",
        "      pred_cls = torch.squeeze(pred_concat.max(1)[1])\n",
        "      train_acc_D += (pred_cls == label_concat).float().mean()\n",
        "\n",
        "\n",
        "      ##########################################################################\n",
        "      ############  2.3 Train Source Encoder w/ FAKE domain label  #############\n",
        "      ##########################################################################\n",
        "      ### Forward only TARGET DOMAIN images through Encoder ###\n",
        "      feat_tgt = encoder(images_tgt)\n",
        "\n",
        "      ### Forward only TARGET DOMAIN features through Discriminator ###\n",
        "      pred_tgt = discriminator(feat_tgt.squeeze(-1).squeeze(-1))     \n",
        "      label_tgt = make_variable(torch.ones(feat_tgt.size(0)).long()) # prepare fake labels\n",
        "      \n",
        "      ### Calculate FAKE domain-classification loss for Encoder ###\n",
        "      loss_DA = criterion_DA(pred_tgt.squeeze(1), label_tgt)\n",
        "      train_loss_E_domain += loss_DA.item()\n",
        "\n",
        "      ### For encoder and Classifier, \n",
        "      ### optimize class-classification & fake domain-classification losses together ###\n",
        "      loss_total = alpha_CLS * loss_CLS +  alpha_DA * loss_DA\n",
        "      loss_total.backward()\n",
        "      optimizer_encoder.step()\n",
        "      optimizer_classifier.step()\n",
        "\n",
        "\n",
        "    #################### 3. Loop through Validation batches ####################\n",
        "    encoder.eval()\n",
        "    classifier.eval()\n",
        "    for data, target in src_data_loaders['valid']:\n",
        "      data = make_variable(data)\n",
        "      target = target.type(torch.FloatTensor).cuda() if multi_label else make_variable(target)\n",
        "      with torch.no_grad():\n",
        "        output = encoder(data)    # [batch_size, n_classes]  (target: [batch_size])\n",
        "        output = torch.flatten(output, 1)\n",
        "        output = classifier(output)\n",
        "\n",
        "      loss = criterion_cls(output, target)\n",
        "      valid_loss_class += loss.item()\n",
        "      if multi_label==False:\n",
        "        output = output.cpu().detach().numpy()\n",
        "        val_n_corr_class += int(sum([np.argmax(pred)==target[i] for i, pred in enumerate(output)]))\n",
        "\n",
        "\n",
        "    ####################  4. Log train/validation losses  ######################\n",
        "    train_acc_D = train_acc_D/min(len(src_data_loaders['train']), len(tgt_data_loader))\n",
        "    print('\\n-----Epoch: %d/%d-----'%(epoch+1, num_epochs))\n",
        "    print('Train Classification Loss (E,C): %.3f  Train Domain Confusion Loss (E): %.3f  Valid Classification Loss (E,C): %.3f'%(train_loss_E_class, train_loss_E_domain, valid_loss_class))  \n",
        "    print('Domain Classification Loss (D): %.3f  Domain Classification Accuracy (D): %.3f  elapsed time: %.1fs'%(train_loss_D, train_acc_D, time.time()-start))  \n",
        "    if multi_label==False:\n",
        "      valid_acc = val_n_corr_class/len(src_data_loaders['valid'].dataset)\n",
        "\n",
        "    ### Reset running losses/accuracies to zero ###\n",
        "    classification_losses_E.append(train_loss_E_class)\n",
        "    domain_confusion_losses_E.append(train_loss_E_domain)\n",
        "    losses_D.append(train_loss_D)\n",
        "    accs_D.append(train_acc_D)\n",
        "    train_loss_E_class, train_loss_E_domain, train_loss_D, running_acc_D, val_n_corr = 0., 0., 0., 0., 0\n",
        "\n",
        "        \n",
        "    #########  5. Show confusion matrices for both domains' test sets  #########\n",
        "    # set threshold=0.5 for source domain confusion matrix \n",
        "    cm = calculate_confusion_matrix(encoder, classifier, transform=transform_, classes=src_data_loaders['train'].dataset.classes, \n",
        "                                    img_dir=src_test_dir, threshold=0.5, multi_label=False)#, test=True)\n",
        "    print(\"--Source Domain Confusion Matrix--\")\n",
        "    print(cm)\n",
        "    # to be more lenient for target domain class deteciton, set threshold to be lower than 0.5 (e.g. 0.2)\n",
        "    cm = calculate_confusion_matrix(encoder, classifier, transform=transform_, classes=tgt_data_loader_small.dataset.classes, \n",
        "                                    img_dir=tgt_test_dir, threshold=test_threshold, multi_label=True)#, test=True)\n",
        "    print(\"--Target Domain Confusion Matrix--\")\n",
        "    print(cm)\n",
        "    print()\n",
        "\n",
        "\n",
        "    ######################  6. Save model checkpoints  #########################\n",
        "    ### Save model if validtion loss is smaller than previous epoch's ###\n",
        "    if valid_loss_class < valid_loss_min:\n",
        "      ### Delete previously saved model checkpoint ###\n",
        "      if prev_save:\n",
        "        os.remove(\"encoder\" + prev_save + \".pt\")\n",
        "        os.remove(\"classifier\" + prev_save + \".pt\")\n",
        "      prev_save = \"_\" + str(epoch+1) \n",
        "\n",
        "      ### Save the new (best) model checkpoints ###\n",
        "      torch.save(encoder.state_dict(), \"encoder\" + prev_save + \".pt\")\n",
        "      torch.save(classifier.state_dict(), \"classifier\" + prev_save + \".pt\")\n",
        "      valid_loss_min = valid_loss_class\n",
        "\n",
        "    ### Regularly save model checkpoints every [save_step] epochs ###\n",
        "    if ((epoch + 1) % save_step == 0):\n",
        "      torch.save(encoder.state_dict(), \"ADDA-encoder-{}.pt\".format(epoch + 1))\n",
        "      torch.save(classifier.state_dict(), \"ADDA-classifier-{}.pt\".format(epoch + 1))\n",
        "\n",
        "  return encoder, classifier, classification_losses_E, domain_confusion_losses_E, losses_D, accs_D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Define Encoder, Classifier, and Discriminator"
      ],
      "metadata": {
        "id": "D3rc_is-XLEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_models(n_classes, pretrained_on=\"stylized_imagenet\"):\n",
        "  assert(pretrained_on in [\"imagenet\", \"stylized_imagenet\"]), 'pretrained_on must be set to one of \"imagenet\" or \"stylized_imagenet\"!'\n",
        "  # For encoder pre-trained on Stylized ImageNet\n",
        "  if pretrained_on==\"stylized_imagenet\":\n",
        "    model = load_model_stylized_imagenet(\"resnet50_trained_on_SIN_and_IN_then_finetuned_on_IN\").module # best performing model from the paper\n",
        "    classifier = nn.Linear(in_features=2048, out_features=n_classes, bias=True)\n",
        "\n",
        "  # For encoder pre-trained on ImageNet\n",
        "  elif pretrained_on==\"imagenet\":\n",
        "    model = models.resnet50(pretrained = True)\n",
        "    classifier = nn.Linear(model.fc.in_features, n_classes) \n",
        "\n",
        "  # Define discriminator\n",
        "  encoder = nn.Sequential(*[model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2, model.layer3, model.layer4, model.avgpool])\n",
        "  discriminator = Discriminator(feature_dim=2048)\n",
        "\n",
        "  return encoder, classifier, discriminator"
      ],
      "metadata": {
        "id": "-dezSu07s-cS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(dataloaders_multi_label['train'].dataset.classes)\n",
        "encoder, classifier, discriminator = define_models(n_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "3d4b2731cf7146b2b9b9bc306bbb5d7e",
            "ebc8aa5fc4f14ba1bad148ed421f20f4",
            "56f1ef1d8ba24e04b23f8f5437388018",
            "5e645311fbf3499a98bc3fe6409a97c2",
            "b6232e1415e44d1ba670b67f2c797d1e",
            "972f8e0cdb034cc9ae3df07a8254f232",
            "3f856084f3a847a081192c13d7832745",
            "456753721cc342ea842c4e9a285b271a",
            "8f0aaf78a8584062b14c40d6f0ff7f06",
            "26d38cc43df74ebc919197744535fe8d",
            "6679c3165e6841659b8c4fafc986c7c6"
          ]
        },
        "id": "6fP1hCvfUKde",
        "outputId": "6ac85160-f43c-4cf3-fcd0-bd138b8c0eca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d4b2731cf7146b2b9b9bc306bbb5d7e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 Train ADDA!"
      ],
      "metadata": {
        "id": "N93YwWBPXOaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"classes : {}\".format(train_data_multi_label.classes))\n",
        "encoder, classifier, classification_losses_E, domain_confusion_losses_E, losses_D, accs_D = train_adda(num_epochs = 30,\n",
        "                                                                                                       lr = 2e-6,\n",
        "                                                                                                       save_step = 5,\n",
        "                                                                                                       encoder = encoder, \n",
        "                                                                                                       classifier = classifier,\n",
        "                                                                                                       discriminator = discriminator,\n",
        "                                                                                                       src_data_loaders = dataloaders_multi_label, \n",
        "                                                                                                       tgt_data_loader = xray_dataloader,\n",
        "                                                                                                       tgt_data_loader_small = xray_test_dataloader,\n",
        "                                                                                                       src_test_dir = \"web_3cls+NOcutter/test\", \n",
        "                                                                                                       tgt_test_dir = \"Xray-3cls_small\",\n",
        "                                                                                                       combined_dataloader = combined_dataloader,\n",
        "                                                                                                       alpha_CLS = 1.,\n",
        "                                                                                                       alpha_DA = 1.5,\n",
        "                                                                                                       multi_label = True,\n",
        "                                                                                                       test_threshold=0.2)"
      ],
      "metadata": {
        "id": "vmRP-dK4wMD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}